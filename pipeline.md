# Documentaci√≥n del Pipeline: Clasificaci√≥n de Imager√≠a Motora con AG + TWP + LDA

**Autor:** [Tu Nombre]  
**Fecha:** Noviembre 2025  
**Versi√≥n:** 1.0

---

## üìã Tabla de Contenidos

1. [Introducci√≥n](#introducci√≥n)
2. [Base Te√≥rica](#base-te√≥rica)
3. [Arquitectura del Pipeline](#arquitectura-del-pipeline)
4. [Implementaci√≥n Detallada](#implementaci√≥n-detallada)
5. [Configuraci√≥n y Par√°metros](#configuraci√≥n-y-par√°metros)
6. [Resultados Esperados](#resultados-esperados)
7. [Interpretaci√≥n de Resultados](#interpretaci√≥n-de-resultados)
8. [Referencias](#referencias)

---

## 1. Introducci√≥n

### 1.1 Objetivo del Proyecto

Este proyecto implementa un **sistema de clasificaci√≥n de se√±ales EEG** para detectar **Imager√≠a Motora (MI - Motor Imagery)** utilizando un enfoque h√≠brido que combina:

- **Transformada Wavelet Packet (TWP)** para extracci√≥n de caracter√≠sticas
- **Algoritmo Gen√©tico (AG)** para selecci√≥n √≥ptima de caracter√≠sticas
- **Linear Discriminant Analysis (LDA)** como clasificador

El objetivo es distinguir entre dos estados mentales:
- **Motor Imagery (MI)**: Imaginaci√≥n de movimiento
- **Rest (Reposo)**: Estado de reposo mental

### 1.2 Contexto Cl√≠nico

Este sistema es parte de una **Brain-Computer Interface (BCI)** dise√±ada para aplicaciones de:
- Rehabilitaci√≥n motora post-ACV
- Control de pr√≥tesis rob√≥ticas
- Comunicaci√≥n asistida

---

## 2. Base Te√≥rica

### 2.1 Imager√≠a Motora y Ritmos Sensoriomotores

#### ¬øQu√© es la Imager√≠a Motora?

La **imager√≠a motora** es la simulaci√≥n mental de un movimiento sin ejecuci√≥n f√≠sica real. Durante este proceso, se activan √°reas motoras del cerebro similares a las que se activar√≠an durante el movimiento real.

#### Fen√≥menos EEG Asociados

Durante la imager√≠a motora, se observan dos fen√≥menos principales en el EEG:

1. **Event-Related Desynchronization (ERD)**
   - Disminuci√≥n de la potencia en bandas de frecuencia espec√≠ficas
   - Ocurre en el ritmo **mu (8-12 Hz)** y **beta (12-30 Hz)**
   - Localizaci√≥n: Corteza motora contralateral al movimiento imaginado

2. **Event-Related Synchronization (ERS)**
   - Aumento de la potencia tras la finalizaci√≥n del movimiento
   - Indica retorno al estado de reposo

#### Bandas de Frecuencia Relevantes

| Banda | Frecuencia | Relevancia para MI |
|-------|------------|-------------------|
| **Theta** | 4-8 Hz | Atenci√≥n y memoria de trabajo |
| **Alfa/Mu** | 8-12 Hz | **Cr√≠tico** - Ritmo sensoriomotor |
| **Beta** | 12-30 Hz | **Importante** - Activaci√≥n motora |
| **Gamma** | >30 Hz | Procesamiento cognitivo complejo |

---

### 2.2 Transformada Wavelet Packet (TWP)

#### Fundamento Matem√°tico

La **Transformada Wavelet** descompone una se√±al en componentes tiempo-frecuencia usando funciones base (wavelets) que son versiones escaladas y trasladadas de una wavelet madre:

```
œà(a,b)(t) = 1/‚àöa * œà((t-b)/a)
```

Donde:
- `a`: Factor de escala (inversamente proporcional a la frecuencia)
- `b`: Factor de traslaci√≥n (posici√≥n temporal)

#### Wavelet Packet Transform (WPT)

A diferencia de la Transformada Wavelet Discreta (DWT), que solo descompone las aproximaciones, la **WPT descompone tanto aproximaciones como detalles**, creando un √°rbol binario completo:

```
                    Se√±al Original
                    /            \
            [Aprox. 1]          [Detalle 1]
            /        \          /         \
        [A1.1]    [D1.1]    [A1.2]     [D1.2]
```

#### Niveles de Descomposici√≥n

- **Level 3**: 2¬≥ = 8 nodos ‚Üí **40 caracter√≠sticas** (5 canales √ó 8)
- **Level 4**: 2‚Å¥ = 16 nodos ‚Üí **80 caracter√≠sticas** (5 canales √ó 16)

Cada nodo representa una banda de frecuencia espec√≠fica.

#### Extracci√≥n de Caracter√≠sticas: Energ√≠a

Para cada nodo del nivel m√°s profundo, se calcula la **energ√≠a**:

```
E_i = Œ£ (coeficiente_j)¬≤
```

La energ√≠a captura la potencia de la se√±al en esa banda tiempo-frecuencia espec√≠fica.

#### Wavelets Utilizadas

En este proyecto se usa **Coiflet 3 (coif3)**:

| Wavelet | Caracter√≠sticas | Uso T√≠pico |
|---------|----------------|------------|
| **coif3** | Soporte compacto, sim√©trica | EEG, se√±ales biom√©dicas |
| db4 | Daubechies 4, ortogonal | An√°lisis general |
| sym4 | Symlet 4, casi sim√©trica | Detecci√≥n de eventos |

---

### 2.3 Algoritmo Gen√©tico (AG) para Selecci√≥n de Caracter√≠sticas

#### ¬øPor qu√© Selecci√≥n de Caracter√≠sticas?

Con 40-80 caracter√≠sticas extra√≠das por TWP, surgen problemas:
- **Maldici√≥n de la dimensionalidad**: Pocos datos (60 trials) vs muchas caracter√≠sticas
- **Overfitting**: El modelo memoriza ruido en lugar de aprender patrones
- **Caracter√≠sticas redundantes**: No todas las bandas/canales son informativas

#### Fundamentos de Algoritmos Gen√©ticos

Los AGs simulan la **evoluci√≥n biol√≥gica** para encontrar soluciones √≥ptimas:

1. **Representaci√≥n (Cromosoma)**
   ```
   Individuo = [1, 0, 1, 1, 0, ..., 1]
   ```
   - Longitud = N√∫mero de caracter√≠sticas (40 u 80)
   - `1` = Caracter√≠stica seleccionada
   - `0` = Caracter√≠stica descartada

2. **Funci√≥n de Aptitud (Fitness)**
   ```
   Fitness = Accuracy_CV - (Œª √ó Sparsity √ó 100)
   ```
   
   - **Accuracy_CV**: Precisi√≥n con validaci√≥n cruzada de 5-fold
   - **Œª (lambda)**: Peso de penalizaci√≥n (0.3 - 2.0)
   - **Sparsity**: Fracci√≥n de caracter√≠sticas usadas
   
   **Objetivo dual:**
   - ‚úÖ Maximizar precisi√≥n (primer t√©rmino)
   - ‚úÖ Minimizar n√∫mero de caracter√≠sticas (segundo t√©rmino)

3. **Operadores Gen√©ticos**

   a) **Selecci√≥n** (Tournament):
   - Se eligen 3 individuos al azar
   - El mejor (mayor fitness) pasa a la siguiente generaci√≥n
   
   b) **Cruce** (Two-Point Crossover, 70% probabilidad):
   ```
   Padre 1:  [1 1 0 | 0 1 | 0 1]
   Padre 2:  [0 1 1 | 1 0 | 1 0]
              -------   ---  -----
   Hijo 1:   [1 1 0 | 1 0 | 0 1]
   Hijo 2:   [0 1 1 | 0 1 | 1 0]
   ```
   
   c) **Mutaci√≥n** (Bit-Flip, 30% probabilidad, 5% por gen):
   ```
   Antes: [1 0 1 1 0 0 1]
   Despu√©s: [1 0 0 1 0 0 1]  (flip en posici√≥n 2)
   ```

4. **Early Stopping**
   - Detiene el AG si no hay mejora en **15 generaciones consecutivas**
   - Evita gasto computacional innecesario
   - T√≠picamente converge entre generaciones 30-60

#### Ventajas del AG sobre Otros M√©todos

| M√©todo | Ventajas | Desventajas |
|--------|----------|-------------|
| **Algoritmo Gen√©tico** | B√∫squeda global, escapa m√≠nimos locales | Costoso computacionalmente |
| Filter (ANOVA, Chi¬≤) | R√°pido, simple | No considera interacciones |
| Wrapper (RFE) | Considera clasificador | Solo b√∫squeda local |
| Embedded (Lasso) | Integrado en entrenamiento | Limitado a modelos lineales |

---

### 2.4 Linear Discriminant Analysis (LDA)

#### Fundamento Matem√°tico

LDA busca un **hiperplano de separaci√≥n** que maximice la distancia entre clases y minimice la varianza intra-clase:

```
J(w) = (w^T S_B w) / (w^T S_W w)
```

Donde:
- `S_B`: Matriz de dispersi√≥n entre clases
- `S_W`: Matriz de dispersi√≥n dentro de clases
- `w`: Vector de proyecci√≥n √≥ptimo

#### Ventajas de LDA para BCI

1. **Eficiente**: Soluci√≥n anal√≠tica cerrada (no iterativo)
2. **Robusto**: Funciona bien con pocas muestras
3. **Interpretable**: El vector de pesos indica importancia de caracter√≠sticas
4. **R√°pido**: Clasificaci√≥n en tiempo real (<1 ms)

#### Shrinkage LDA

Para evitar problemas cuando `n_caracter√≠sticas > n_muestras`, se usa **Shrinkage**:

```python
LinearDiscriminantAnalysis(shrinkage='auto', solver='eigen')
```

- **Shrinkage autom√°tico**: Estima el par√°metro de regularizaci√≥n √≥ptimo
- **Solver eigen**: M√°s estable num√©ricamente

---

## 3. Arquitectura del Pipeline

### 3.1 Diagrama de Flujo General

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DATOS CRUDOS EEG                         ‚îÇ
‚îÇ          (8 sujetos, 5 canales, 250 Hz)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              PREPROCESAMIENTO                                ‚îÇ
‚îÇ  - Filtro Butterworth (8-30 Hz)                            ‚îÇ
‚îÇ  - Segmentaci√≥n en trials (2.5s MI + 2.5s Rest)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         EXTRACCI√ìN DE CARACTER√çSTICAS (TWP)                 ‚îÇ
‚îÇ  - Transformada Wavelet Packet (level=3, coif3)            ‚îÇ
‚îÇ  - C√°lculo de energ√≠a por nodo                             ‚îÇ
‚îÇ  - Resultado: 60 trials √ó 40 caracter√≠sticas               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        SELECCI√ìN DE CARACTER√çSTICAS (AG)                    ‚îÇ
‚îÇ  - Poblaci√≥n: 100 individuos                               ‚îÇ
‚îÇ  - Generaciones: hasta 100 (con early stopping)           ‚îÇ
‚îÇ  - Fitness: Accuracy_CV - Œª√óSparsity                      ‚îÇ
‚îÇ  - Resultado: ~3-5 caracter√≠sticas √≥ptimas                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              ENTRENAMIENTO (LDA)                            ‚îÇ
‚îÇ  - Datos de calibraci√≥n (60 trials)                        ‚îÇ
‚îÇ  - Solo caracter√≠sticas seleccionadas por AG               ‚îÇ
‚îÇ  - Shrinkage autom√°tico                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              EVALUACI√ìN (Terapia)                           ‚îÇ
‚îÇ  - Datos de terapia (60 trials nuevos)                     ‚îÇ
‚îÇ  - Mismas caracter√≠sticas que en entrenamiento             ‚îÇ
‚îÇ  - M√©tricas: Accuracy, TPR (Sensibilidad)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### 3.2 Estructura de Archivos

```
proyecto/
‚îÇ
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ file_creation.py          # Carga y preprocesamiento
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ filtering.py               # Filtros Butterworth
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ feature_extraction/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ feature_extraction.py      # TWP, PSD
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ genetic_algorithm/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ genetic_algorithm.py       # AG con DEAP
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ training.py                # Entrenamiento LDA
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py              # Evaluaci√≥n de modelos
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ metrics/
‚îÇ       ‚îî‚îÄ‚îÄ metrics.py                 # Accuracy, TPR, F1, etc.
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ im_tention_signals/            # Datos EEG (.mat)
‚îÇ
‚îú‚îÄ‚îÄ Pruebas_TWP.ipynb                  # Notebook principal
‚îÇ
‚îî‚îÄ‚îÄ resultados_ag_twp_lda.csv          # Resultados exportados
```

---

## 4. Implementaci√≥n Detallada

### 4.1 FASE 1: Carga y Preprocesamiento

#### C√≥digo:
```python
dict_cal = create_mat_files(
    './data/im_tention_signals', 
    file_type='calibration', 
    filtfilt=True
)

dict_ter = create_mat_files(
    './data/im_tention_signals', 
    file_type='therapy', 
    ther_number_of_trials=60, 
    filtfilt=True
)
```

#### Procesamiento Interno:

1. **Filtrado Butterworth**:
   - Orden: 4
   - Banda de paso: 8-30 Hz
   - Tipo: Pasa-banda
   - M√©todo: `filtfilt` (fase cero, sin distorsi√≥n temporal)

2. **Segmentaci√≥n de Trials**:
   ```
   Timeline de un trial:
   
   ‚Üê0.5s‚Üí     ‚Üê‚îÄ 2.5s ‚îÄ‚Üí    ‚Üê0.5s‚Üí     ‚Üê‚îÄ 2.5s ‚îÄ‚Üí
   [Skip] [Motor Imagery] [Skip]     [Rest]
            ‚Üë
          Marca MI
   ```
   
   - **Skip**: 0.5s despu√©s/antes de marca (evitar artefactos)
   - **MI**: 2.5s de imaginaci√≥n motora (625 muestras a 250 Hz)
   - **Rest**: 2.5s de reposo antes de la marca

3. **Estructura de Datos**:
   ```python
   dict_cal['subject_1']['mi_rest'].shape  # (625, 5, 60)
   # 625 muestras √ó 5 canales √ó 60 trials (30 MI + 30 Rest)
   
   dict_ter['subject_1']['mi'].shape       # (625, 5, 60)
   dict_ter['subject_1']['target']         # (60,) - 0/1 targets
   ```

---

### 4.2 FASE 2: Extracci√≥n de Caracter√≠sticas (TWP)

#### C√≥digo:
```python
X_cal_twp = get_twp_feature_vectors(
    data_cal, 
    level=3,           # 8 nodos por canal
    wavelet='coif3',   # Coiflet 3
    normalize=False,   # Sin normalizaci√≥n
    log_transform=False
)
# Resultado: (60, 40) ‚Üí 60 trials √ó 40 caracter√≠sticas
```

#### Proceso Detallado:

1. **Por cada trial (60 en total)**:
   
   a) **Por cada canal (5 canales: C3, Cz, C4, P3, Pz)**:
      - Extraer se√±al temporal del canal: `(625,)` muestras
   
   b) **Descomposici√≥n Wavelet Packet**:
      ```python
      wp = pywt.WaveletPacket(
          data=signal_channel,
          wavelet='coif3',
          mode='symmetric',
          maxlevel=3
      )
      ```
      
   c) **Extraer nodos del nivel 3**:
      - Obtener 8 nodos (2¬≥ = 8)
      - Cada nodo contiene ~78 coeficientes wavelet
   
   d) **Calcular energ√≠a por nodo**:
      ```python
      for node in nodes_level_3:
          energy = np.sum(np.square(node.data))
      ```
      - Resultado: 8 valores de energ√≠a por canal

2. **Concatenaci√≥n**:
   - 5 canales √ó 8 nodos = **40 caracter√≠sticas por trial**
   - Matriz final: `(60 trials, 40 features)`

#### Interpretaci√≥n de Caracter√≠sticas:

Cada caracter√≠stica representa la **energ√≠a en una banda tiempo-frecuencia espec√≠fica** de un canal determinado:

```
Feature 0:  Canal C3, Nodo 0 (frecuencias m√°s bajas)
Feature 1:  Canal C3, Nodo 1
...
Feature 7:  Canal C3, Nodo 7 (frecuencias m√°s altas)
Feature 8:  Canal Cz, Nodo 0
...
Feature 39: Canal Pz, Nodo 7
```

---

### 4.3 FASE 3: Selecci√≥n de Caracter√≠sticas (AG)

#### C√≥digo:
```python
best_individual, logbook = run_genetic_algorithm(
    X_cal_twp,              # (60, 40)
    targets_cal,            # (60,) [1,1,...,0,0,...]
    pop_size=100,
    num_generations=100,
    lambda_penalty=1.0,
    early_stopping_patience=15
)

selected_indices = [idx for idx, bit in enumerate(best_individual) if bit == 1]
# Ejemplo: selected_indices = [8, 16, 24, 39] ‚Üí 4 caracter√≠sticas
```

#### Pseudoc√≥digo del Algoritmo:

```
ALGORITMO GEN√âTICO(X, y, pop_size, num_gen, Œª)
‚îÇ
‚îú‚îÄ‚îÄ Inicializar poblaci√≥n aleatoria de 100 individuos
‚îÇ   Individuo = vector binario de longitud 40
‚îÇ
‚îú‚îÄ‚îÄ PARA generaci√≥n = 1 hasta 100:
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ PARA CADA individuo en poblaci√≥n:
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Extraer caracter√≠sticas seleccionadas (1s en el vector)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SI no hay caracter√≠sticas seleccionadas:
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fitness = 0
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SINO:
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ X_reducido = X[:, caracter√≠sticas_seleccionadas]
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LDA = LinearDiscriminantAnalysis()
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scores = cross_val_score(LDA, X_reducido, y, cv=5)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ accuracy = mean(scores) √ó 100
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sparsity = n_seleccionadas / 40
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ penalizaci√≥n = Œª √ó sparsity √ó 100
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fitness = accuracy - penalizaci√≥n
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Asignar fitness al individuo
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Seleccionar mejores individuos (Tournament)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Aplicar Cruce (70% probabilidad, two-point)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Aplicar Mutaci√≥n (30% probabilidad, bit-flip 5%)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ SI fitness_m√°ximo no mejor√≥ en 15 generaciones:
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ BREAK (Early stopping)
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Registrar estad√≠sticas (max, avg, std)
‚îÇ
‚îî‚îÄ‚îÄ RETORNAR mejor individuo encontrado
```

#### Ejemplo de Evoluci√≥n:

```
Gen   0: Max Fitness =  35.83 | Avg =  15.33 | Std =  8.68
         ‚Üí Poblaci√≥n inicial aleatoria, baja aptitud

Gen  10: Max Fitness =  62.50 | Avg =  48.67 | Std =  4.87
         ‚Üí Convergencia r√°pida, mejora significativa

Gen  30: Max Fitness =  68.33 | Avg =  63.53 | Std =  5.04
         ‚Üí Cerca del √≥ptimo, poblaci√≥n homog√©nea

Gen  40: Max Fitness =  68.33 | Avg =  64.39 | Std =  6.31
         ‚Üí Sin mejora por 15 generaciones ‚Üí EARLY STOP
```

#### An√°lisis del Mejor Individuo:

```python
best_individual = [0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,...]
                                    ‚Üë              ‚Üë              ‚Üë
                                Feature 8      Feature 16     Feature 24
                                (Cz, Nodo 0)   (C4, Nodo 0)   (P3, Nodo 0)
```

**Interpretaci√≥n**: El AG seleccion√≥ caracter√≠sticas de nodos de baja frecuencia (Nodo 0) en canales centrales y parietales.

---

### 4.4 FASE 4: Entrenamiento del Clasificador

#### C√≥digo:
```python
X_cal_opt = X_cal_twp[:, selected_indices]  # (60, 4) si 4 features

clf = LinearDiscriminantAnalysis(shrinkage='auto', solver='eigen')
clf_final, metrics_cal = train_clf_and_get_metrics(
    X_cal_opt, 
    y_calibration, 
    clf
)

print(f"Calibraci√≥n - Acc: {metrics_cal.acc}% | TPR: {metrics_cal.tpr}%")
```

#### Proceso Interno:

1. **Reducci√≥n de Dimensionalidad**:
   - Entrada: `(60, 40)` ‚Üí Salida: `(60, 4)`
   - Se usan solo las columnas seleccionadas por el AG

2. **Entrenamiento LDA**:
   ```python
   clf.fit(X_cal_opt, y_calibration)
   ```
   
   Internamente:
   - Calcula medias por clase: `Œº_MI`, `Œº_Rest`
   - Calcula matrices de covarianza: `Œ£_MI`, `Œ£_Rest`
   - Aplica shrinkage: `Œ£_shrunk = (1-Œ±)Œ£ + Œ±¬∑I`
   - Resuelve problema de autovalores para encontrar `w`

3. **M√©tricas en Calibraci√≥n**:
   ```python
   y_pred = clf.predict(X_cal_opt)
   
   accuracy = np.sum(y_pred == y_true) / len(y_true) √ó 100
   
   tpr = np.sum(y_pred[y_true == 1] == 1) / np.sum(y_true == 1) √ó 100
   ```

---

### 4.5 FASE 5: Evaluaci√≥n en Terapia

#### C√≥digo:
```python
# 1. Extraer caracter√≠sticas de datos de terapia
X_ter_twp = get_twp_feature_vectors(
    data_ter, 
    level=3, 
    wavelet='coif3'
)  # (60, 40)

# 2. Aplicar LA MISMA selecci√≥n de caracter√≠sticas
X_ter_opt = X_ter_twp[:, selected_indices]  # (60, 4)

# 3. Clasificar con el modelo entrenado
y_pred = clf_final.predict(X_ter_opt)

# 4. Evaluar contra targets reales
metrics_ter = evaluate_clf_and_get_metrics(
    X_ter_opt, 
    clf_final, 
    targets_ter  # Targets reales de terapia
)

print(f"Terapia - Acc: {metrics_ter.acc}% | TPR: {metrics_ter.tpr}%")
```

#### ‚ö†Ô∏è IMPORTANTE: Mismo Pipeline de Preprocesamiento

Es **cr√≠tico** que los datos de terapia pasen por el **mismo pipeline**:

1. ‚úÖ Mismo filtro (8-30 Hz, Butterworth orden 4)
2. ‚úÖ Misma wavelet (coif3)
3. ‚úÖ Mismo level (3)
4. ‚úÖ **Mismas caracter√≠sticas** (√≠ndices del AG)

Si cambias cualquiera de estos pasos, los resultados ser√°n inv√°lidos.

---

## 5. Configuraci√≥n y Par√°metros

### 5.1 Par√°metros del Pipeline

```python
# ============== TRANSFORMADA WAVELET PACKET ==============
TWP_LEVEL = 3              # Nivel de descomposici√≥n
                           # 3 ‚Üí 40 features | 4 ‚Üí 80 features

TWP_WAVELET = 'coif3'      # Familia de wavelet
                           # Opciones: 'db4', 'coif3', 'sym4', 'bior3.3'

# ============== ALGORITMO GEN√âTICO ==============
POP_SIZE = 100             # Tama√±o de la poblaci√≥n
                           # Rango recomendado: 50-200

N_GEN = 100                # N√∫mero m√°ximo de generaciones
                           # T√≠picamente converge en 30-60

LAMBDA_PENALTY = 1.0       # Peso de penalizaci√≥n por sparsity
                           # ‚Üë Mayor ‚Üí Menos caracter√≠sticas
                           # ‚Üì Menor ‚Üí M√°s caracter√≠sticas
                           # Rango: 0.3 - 2.0

EARLY_STOPPING = 15        # Paciencia para early stopping
                           # Detiene si no mejora en N generaciones

K_FOLDS = 5                # Folds para validaci√≥n cruzada
                           # Usado en el fitness del AG

# ============== CLASIFICADOR LDA ==============
SHRINKAGE = 'auto'         # Regularizaci√≥n autom√°tica
SOLVER = 'eigen'           # Solver m√°s estable

# ============== PREPROCESAMIENTO ==============
FILTER_LOW = 8             # Frecuencia baja del filtro (Hz)
FILTER_HIGH = 30           # Frecuencia alta del filtro (Hz)
FILTER_ORDER = 4           # Orden del filtro Butterworth
SAMPLING_RATE = 250        # Frecuencia de muestreo (Hz)
```

### 5.2 Gu√≠a de Ajuste de Par√°metros

#### Lambda (LAMBDA_PENALTY)

| Valor | Efecto | Cu√°ndo Usar |
|-------|--------|-------------|
| 0.1-0.3 | Selecci√≥n **permisiva** (10-15 features) | Dataset peque√±o, se√±ales ruidosas |
| 0.5-1.0 | Selecci√≥n **balanceada** (5-8 features) | **Recomendado** para inicio |
| 1.5-2.5 | Selecci√≥n **agresiva** (2-4 features) | Muchas caracter√≠sticas redundantes |
| >3.0 | **Demasiado restrictivo** (1-2 features) | ‚ö†Ô∏è No recomendado (underfitting) |

**Regla pr√°ctica:**
```
Si (Acc_Calibraci√≥n - Acc_Terapia) > 20%:
    ‚Üí Reducir lambda (menos overfitting)

Si n_caracter√≠sticas_promedio < 3:
    ‚Üí Reducir lambda (m√°s caracter√≠sticas)

Si TPR_Terapia < 50%:
    ‚Üí Reducir lambda o aumentar level
```

#### Level de TWP

| Level | N¬∞ Features | Resoluci√≥n Frecuencia | Uso Recomendado |
|-------|-------------|----------------------|-----------------|
| 2 | 20 (5√ó4) | Baja | Pruebas r√°pidas |
| **3** | **40 (5√ó8)** | **Media** | **Est√°ndar** |
| **4** | **80 (5√ó16)** | **Alta** | Mayor precisi√≥n |
| 5 | 160 (5√ó32) | Muy alta | ‚ö†Ô∏è Riesgo overfitting |

---

## 6. Resultados Esperados

### 6.1 M√©tricas T√≠picas

#### Calibraci√≥n (Training)
- **Accuracy**: 70-85%
- **TPR**: 75-90%
- **Gap calibraci√≥n-terapia**: 10-20%

#### Terapia (Test/Validaci√≥n)
- **Accuracy**: 55-70%
- **TPR**: 55-75%

#### Selecci√≥n de Caracter√≠sticas
- **Promedio seleccionadas**: 3-8 de 40 (7.5-20%)
- **Reducci√≥n**: 80-92.5%

### 6.2 Ejemplo de Resultados (Tu Ejecuci√≥n)

```
RESUMEN ESTAD√çSTICO:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
M√©trica                        Media        Std
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Caracter√≠sticas seleccionadas  2.88         1.36
Porcentaje selecci√≥n (%)       7.19         3.39
Fitness AG                     68.23        6.38
Acc Calibraci√≥n (%)            74.17        8.20
TPR Calibraci√≥n (%)            79.58        8.66
Acc Terapia (%)                56.88        6.38
TPR Terapia (%)                60.25        29.37
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
```

### 6.3 Comparaci√≥n con Baseline (Sin AG)

| M√©trica | Baseline (40 features) | Con AG (2.9 features