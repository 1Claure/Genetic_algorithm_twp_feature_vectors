# Documentaci√≥n del Pipeline: Clasificaci√≥n de imaginaci√≥n motora con AG + TWP + LDA

**Autor:** Claure Jorge, Diana Vertiz  
**Fecha:** Noviembre 2025  
**Versi√≥n:** 1.0

---

## üìã Tabla de Contenidos

1. [Introducci√≥n](#Introducci√≥n)
2. [Base Te√≥rica](#base-te√≥rica)
3. [Arquitectura del Pipeline](#arquitectura-del-pipeline)
4. [Implementaci√≥n Detallada](#implementaci√≥n-detallada)
5. [Configuraci√≥n y Par√°metros](#configuraci√≥n-y-par√°metros)
6. [Resultados Esperados](#resultados-esperados)
7. [Interpretaci√≥n de Resultados](#interpretaci√≥n-de-resultados)
8. [Referencias](#referencias)

---

## 1. Introducci√≥n

### 1.1 Objetivo del Proyecto

Este proyecto implementa un **sistema de clasificaci√≥n de se√±ales EEG** para detectar **Imaginaci√≥n Motora (MI - Motor Imagery)** utilizando un enfoque h√≠brido que combina:

- **Transformada Wavelet Packet (TWP)** para extracci√≥n de caracter√≠sticas
- **Algoritmo Gen√©tico (AG)** para selecci√≥n √≥ptima de caracter√≠sticas
- **Linear Discriminant Analysis (LDA)** como clasificador

El objetivo es distinguir entre dos estados mentales:
- **Motor Imagery (MI)**: Imaginaci√≥n de movimiento
- **Rest (Reposo)**: Estado de reposo mental

### 1.2 Contexto Cl√≠nico

Este sistema es parte de una **Brain-Computer Interface (BCI)** dise√±ada para aplicaciones de:
- Rehabilitaci√≥n motora
- Control de pr√≥tesis rob√≥ticas

---

## 2. Base Te√≥rica

### 2.1 Imaginaci√≥n motora y ritmos sensoriomotores

#### ¬øQu√© es la imaginaci√≥n motora?

La **imaginaci√≥n motora** es la simulaci√≥n mental de un movimiento sin ejecuci√≥n f√≠sica real. Durante este proceso, se activan √°reas motoras del cerebro similares a las que se activar√≠an durante el movimiento real.

#### Fen√≥menos EEG Asociados

Durante la imager√≠a motora, se observan dos fen√≥menos principales en el EEG:

1. **Event-Related Desynchronization (ERD)**
   - Disminuci√≥n de la potencia en bandas de frecuencia espec√≠ficas
   - Ocurre en el ritmo **mu (8-12 Hz)** y **beta (12-30 Hz)**
   - Localizaci√≥n: Corteza motora contralateral al movimiento imaginado

2. **Event-Related Synchronization (ERS)**
   - Aumento de la potencia tras la finalizaci√≥n del movimiento
   - Indica retorno al estado de reposo

#### Bandas de Frecuencia Relevantes

| Banda | Frecuencia | Relevancia para MI |
|-------|------------|-------------------|
| **Theta** | 4-8 Hz | Atenci√≥n y memoria de trabajo |
| **Alfa/Mu** | 8-12 Hz | **Cr√≠tico** - Ritmo sensoriomotor |
| **Beta** | 12-30 Hz | **Importante** - Activaci√≥n motora |
| **Gamma** | >30 Hz | Procesamiento cognitivo complejo |

---

### 2.2 Transformada Wavelet Packet (TWP)

#### Fundamento matem√°tico

La **Transformada Wavelet** descompone una se√±al en componentes tiempo-frecuencia usando funciones base (wavelets) que son versiones escaladas y trasladadas de una wavelet madre:

```
œà(a,b)(t) = 1/‚àöa * œà((t-b)/a)
```

Donde:
- `a`: Factor de escala (inversamente proporcional a la frecuencia)
- `b`: Factor de traslaci√≥n (posici√≥n temporal)

#### Wavelet Packet Transform (WPT)

A diferencia de la Transformada Wavelet Discreta (DWT), que solo descompone las aproximaciones, la **WPT descompone tanto aproximaciones como detalles**, creando un √°rbol binario completo:

```
                    Se√±al Original
                    /            \
            [Aprox. 1]          [Detalle 1]
            /        \          /         \
        [A1.1]    [D1.1]    [A1.2]     [D1.2]
```

#### Niveles de Descomposici√≥n

- **Level 3**: 2¬≥ = 8 nodos ‚Üí **40 caracter√≠sticas** (5 canales √ó 8)
- **Level 4**: 2‚Å¥ = 16 nodos ‚Üí **80 caracter√≠sticas** (5 canales √ó 16)

Cada nodo representa una banda de frecuencia espec√≠fica.

#### Extracci√≥n de Caracter√≠sticas: Energ√≠a

Para cada nodo del nivel m√°s profundo, se calcula la **energ√≠a**:

```
E_i = Œ£ (coeficiente_j)¬≤
```

La energ√≠a captura la potencia de la se√±al en esa banda tiempo-frecuencia espec√≠fica.

#### Wavelets Utilizadas

En este proyecto se usa **Coiflet 3 (coif3)**:

| Wavelet | Caracter√≠sticas | Uso T√≠pico |
|---------|----------------|------------|
| **coif3** | Soporte compacto, sim√©trica | EEG, se√±ales biom√©dicas |
| db4 | Daubechies 4, ortogonal | An√°lisis general |
| sym4 | Symlet 4, casi sim√©trica | Detecci√≥n de eventos |

---

### 2.3 Algoritmo Gen√©tico (AG) para Selecci√≥n de Caracter√≠sticas

#### ¬øPor qu√© Selecci√≥n de Caracter√≠sticas?

Con 40-80 caracter√≠sticas extra√≠das por TWP, surgen problemas:
- **Maldici√≥n de la dimensionalidad**: Pocos datos (60 trials) vs muchas caracter√≠sticas
- **Overfitting**: El modelo memoriza ruido en lugar de aprender patrones
- **Caracter√≠sticas redundantes**: No todas las bandas/canales son informativas

#### Fundamentos de Algoritmos Gen√©ticos

Los AGs simulan la **evoluci√≥n biol√≥gica** para encontrar soluciones √≥ptimas:

1. **Representaci√≥n (Cromosoma)**
   ```
   Individuo = [1, 0, 1, 1, 0, ..., 1]
   ```
   - Longitud = N√∫mero de caracter√≠sticas (40 u 80)
   - `1` = Caracter√≠stica seleccionada
   - `0` = Caracter√≠stica descartada

2. **Funci√≥n de Aptitud (Fitness)**
   ```
   Fitness = Accuracy_CV - (Œª √ó Sparsity √ó 100)
   ```
   
   - **Accuracy_CV**: Precisi√≥n con validaci√≥n cruzada de 5-fold
   - **Œª (lambda)**: Peso de penalizaci√≥n (0.3 - 2.0)
   - **Sparsity**: Fracci√≥n de caracter√≠sticas usadas
   
   **Objetivo dual:**
   - ‚úÖ Maximizar precisi√≥n (primer t√©rmino)
   - ‚úÖ Minimizar n√∫mero de caracter√≠sticas (segundo t√©rmino)

3. **Operadores Gen√©ticos**

   a) **Selecci√≥n** (Tournament):
   - Se eligen 3 individuos al azar
   - El mejor (mayor fitness) pasa a la siguiente generaci√≥n
   
   b) **Cruce** (Two-Point Crossover, 70% probabilidad):
   ```
   Padre 1:  [1 1 0 | 0 1 | 0 1]
   Padre 2:  [0 1 1 | 1 0 | 1 0]
              -------   ---  -----
   Hijo 1:   [1 1 0 | 1 0 | 0 1]
   Hijo 2:   [0 1 1 | 0 1 | 1 0]
   ```
   
   c) **Mutaci√≥n** (Bit-Flip, 30% probabilidad, 5% por gen):
   ```
   Antes: [1 0 1 1 0 0 1]
   Despu√©s: [1 0 0 1 0 0 1]  (flip en posici√≥n 2)
   ```

4. **Early Stopping**
   - Detiene el AG si no hay mejora en **15 generaciones consecutivas**
   - Evita gasto computacional innecesario
   - T√≠picamente converge entre generaciones 30-60

#### Ventajas del AG sobre Otros M√©todos

| M√©todo | Ventajas | Desventajas |
|--------|----------|-------------|
| **Algoritmo Gen√©tico** | B√∫squeda global, escapa m√≠nimos locales | Costoso computacionalmente |
| Filter (ANOVA, Chi¬≤) | R√°pido, simple | No considera interacciones |
| Wrapper (RFE) | Considera clasificador | Solo b√∫squeda local |
| Embedded (Lasso) | Integrado en entrenamiento | Limitado a modelos lineales |

---

### 2.4 Linear Discriminant Analysis (LDA)

#### Fundamento Matem√°tico

LDA busca un **hiperplano de separaci√≥n** que maximice la distancia entre clases y minimice la varianza intra-clase:

```
J(w) = (w^T S_B w) / (w^T S_W w)
```

Donde:
- `S_B`: Matriz de dispersi√≥n entre clases
- `S_W`: Matriz de dispersi√≥n dentro de clases
- `w`: Vector de proyecci√≥n √≥ptimo

#### Ventajas de LDA para BCI

1. **Eficiente**: Soluci√≥n anal√≠tica cerrada (no iterativo)
2. **Robusto**: Funciona bien con pocas muestras
3. **Interpretable**: El vector de pesos indica importancia de caracter√≠sticas
4. **R√°pido**: Clasificaci√≥n en tiempo real (<1 ms)

#### Shrinkage LDA

Para evitar problemas cuando `n_caracter√≠sticas > n_muestras`, se usa **Shrinkage**:

```python
LinearDiscriminantAnalysis(shrinkage='auto', solver='eigen')
```

- **Shrinkage autom√°tico**: Estima el par√°metro de regularizaci√≥n √≥ptimo
- **Solver eigen**: M√°s estable num√©ricamente

---

## 3. Arquitectura del Pipeline

### 3.1 Diagrama de Flujo General

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    DATOS CRUDOS EEG                         ‚îÇ
‚îÇ          (8 sujetos, 5 canales, 250 Hz)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              PREPROCESAMIENTO                                ‚îÇ
‚îÇ  - Filtro Butterworth (8-30 Hz)                            ‚îÇ
‚îÇ  - Segmentaci√≥n en trials (2.5s MI + 2.5s Rest)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         EXTRACCI√ìN DE CARACTER√çSTICAS (TWP)                 ‚îÇ
‚îÇ  - Transformada Wavelet Packet (level=3, coif3)            ‚îÇ
‚îÇ  - C√°lculo de energ√≠a por nodo                             ‚îÇ
‚îÇ  - Resultado: 60 trials √ó 40 caracter√≠sticas               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        SELECCI√ìN DE CARACTER√çSTICAS (AG)                    ‚îÇ
‚îÇ  - Poblaci√≥n: 100 individuos                               ‚îÇ
‚îÇ  - Generaciones: hasta 100 (con early stopping)           ‚îÇ
‚îÇ  - Fitness: Accuracy_CV - Œª√óSparsity                      ‚îÇ
‚îÇ  - Resultado: ~3-5 caracter√≠sticas √≥ptimas                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              ENTRENAMIENTO (LDA)                            ‚îÇ
‚îÇ  - Datos de calibraci√≥n (60 trials)                        ‚îÇ
‚îÇ  - Solo caracter√≠sticas seleccionadas por AG               ‚îÇ
‚îÇ  - Shrinkage autom√°tico                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              EVALUACI√ìN (Terapia)                           ‚îÇ
‚îÇ  - Datos de terapia (60 trials nuevos)                     ‚îÇ
‚îÇ  - Mismas caracter√≠sticas que en entrenamiento             ‚îÇ
‚îÇ  - M√©tricas: Accuracy, TPR (Sensibilidad)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### 3.2 Estructura de Archivos

```
proyecto/
‚îÇ
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ file_creation.py          # Carga y preprocesamiento
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ filtering.py               # Filtros Butterworth
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ feature_extraction/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ feature_extraction.py      # TWP, PSD
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ genetic_algorithm/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ genetic_algorithm.py       # AG con DEAP
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ training.py                # Entrenamiento LDA
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py              # Evaluaci√≥n de modelos
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ metrics/
‚îÇ       ‚îî‚îÄ‚îÄ metrics.py                 # Accuracy, TPR, F1, etc.
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ im_tention_signals/            # Datos EEG (.mat)
‚îÇ
‚îú‚îÄ‚îÄ Pruebas_TWP.ipynb                  # Notebook principal
‚îÇ
‚îî‚îÄ‚îÄ resultados_ag_twp_lda.csv          # Resultados exportados
```

---

## 4. Implementaci√≥n Detallada

### 4.1 FASE 1: Carga y Preprocesamiento

#### C√≥digo:
```python
dict_cal = create_mat_files(
    './data/im_tention_signals', 
    file_type='calibration', 
    filtfilt=True
)

dict_ter = create_mat_files(
    './data/im_tention_signals', 
    file_type='therapy', 
    ther_number_of_trials=60, 
    filtfilt=True
)
```

#### Procesamiento Interno:

1. **Filtrado Butterworth**:
   - Orden: 4
   - Banda de paso: 8-30 Hz
   - Tipo: Pasa-banda
   - M√©todo: `filtfilt` (fase cero, sin distorsi√≥n temporal)

2. **Segmentaci√≥n de Trials**:
   ```
   Timeline de un trial:
   
   ‚Üê0.5s‚Üí     ‚Üê‚îÄ 2.5s ‚îÄ‚Üí    ‚Üê0.5s‚Üí     ‚Üê‚îÄ 2.5s ‚îÄ‚Üí
   [Skip] [Motor Imagery] [Skip]     [Rest]
            ‚Üë
          Marca MI
   ```
   
   - **Skip**: 0.5s despu√©s/antes de marca (evitar artefactos)
   - **MI**: 2.5s de imaginaci√≥n motora (625 muestras a 250 Hz)
   - **Rest**: 2.5s de reposo antes de la marca

3. **Estructura de Datos**:
   ```python
   dict_cal['subject_1']['mi_rest'].shape  # (625, 5, 60)
   # 625 muestras √ó 5 canales √ó 60 trials (30 MI + 30 Rest)
   
   dict_ter['subject_1']['mi'].shape       # (625, 5, 60)
   dict_ter['subject_1']['target']         # (60,) - 0/1 targets
   ```

---

### 4.2 FASE 2: Extracci√≥n de Caracter√≠sticas (TWP)

#### C√≥digo:
```python
X_cal_twp = get_twp_feature_vectors(
    data_cal, 
    level=3,           # 8 nodos por canal
    wavelet='coif3',   # Coiflet 3
    normalize=False,   # Sin normalizaci√≥n
    log_transform=False
)
# Resultado: (60, 40) ‚Üí 60 trials √ó 40 caracter√≠sticas
```

#### Proceso Detallado:

1. **Por cada trial (60 en total)**:
   
   a) **Por cada canal (5 canales: C3, Cz, C4, P3, Pz)**:
      - Extraer se√±al temporal del canal: `(625,)` muestras
   
   b) **Descomposici√≥n Wavelet Packet**:
      ```python
      wp = pywt.WaveletPacket(
          data=signal_channel,
          wavelet='coif3',
          mode='symmetric',
          maxlevel=3
      )
      ```
      
   c) **Extraer nodos del nivel 3**:
      - Obtener 8 nodos (2¬≥ = 8)
      - Cada nodo contiene ~78 coeficientes wavelet
   
   d) **Calcular energ√≠a por nodo**:
      ```python
      for node in nodes_level_3:
          energy = np.sum(np.square(node.data))
      ```
      - Resultado: 8 valores de energ√≠a por canal

2. **Concatenaci√≥n**:
   - 5 canales √ó 8 nodos = **40 caracter√≠sticas por trial**
   - Matriz final: `(60 trials, 40 features)`

#### Interpretaci√≥n de Caracter√≠sticas:

Cada caracter√≠stica representa la **energ√≠a en una banda tiempo-frecuencia espec√≠fica** de un canal determinado:

```
Feature 0:  Canal C3, Nodo 0 (frecuencias m√°s bajas)
Feature 1:  Canal C3, Nodo 1
...
Feature 7:  Canal C3, Nodo 7 (frecuencias m√°s altas)
Feature 8:  Canal Cz, Nodo 0
...
Feature 39: Canal Pz, Nodo 7
```

---

### 4.3 FASE 3: Selecci√≥n de Caracter√≠sticas (AG)

#### C√≥digo:
```python
best_individual, logbook = run_genetic_algorithm(
    X_cal_twp,              # (60, 40)
    targets_cal,            # (60,) [1,1,...,0,0,...]
    pop_size=100,
    num_generations=100,
    lambda_penalty=1.0,
    early_stopping_patience=15
)

selected_indices = [idx for idx, bit in enumerate(best_individual) if bit == 1]
# Ejemplo: selected_indices = [8, 16, 24, 39] ‚Üí 4 caracter√≠sticas
```

#### Pseudoc√≥digo del Algoritmo:

```
ALGORITMO GEN√âTICO(X, y, pop_size, num_gen, Œª)
‚îÇ
‚îú‚îÄ‚îÄ Inicializar poblaci√≥n aleatoria de 100 individuos
‚îÇ   Individuo = vector binario de longitud 40
‚îÇ
‚îú‚îÄ‚îÄ PARA generaci√≥n = 1 hasta 100:
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ PARA CADA individuo en poblaci√≥n:
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Extraer caracter√≠sticas seleccionadas (1s en el vector)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SI no hay caracter√≠sticas seleccionadas:
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fitness = 0
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SINO:
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ X_reducido = X[:, caracter√≠sticas_seleccionadas]
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LDA = LinearDiscriminantAnalysis()
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scores = cross_val_score(LDA, X_reducido, y, cv=5)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ accuracy = mean(scores) √ó 100
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sparsity = n_seleccionadas / 40
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ penalizaci√≥n = Œª √ó sparsity √ó 100
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fitness = accuracy - penalizaci√≥n
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Asignar fitness al individuo
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Seleccionar mejores individuos (Tournament)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Aplicar Cruce (70% probabilidad, two-point)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Aplicar Mutaci√≥n (30% probabilidad, bit-flip 5%)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ SI fitness_m√°ximo no mejor√≥ en 15 generaciones:
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ BREAK (Early stopping)
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Registrar estad√≠sticas (max, avg, std)
‚îÇ
‚îî‚îÄ‚îÄ RETORNAR mejor individuo encontrado
```

#### Ejemplo de Evoluci√≥n:

```
Gen   0: Max Fitness =  35.83 | Avg =  15.33 | Std =  8.68
         ‚Üí Poblaci√≥n inicial aleatoria, baja aptitud

Gen  10: Max Fitness =  62.50 | Avg =  48.67 | Std =  4.87
         ‚Üí Convergencia r√°pida, mejora significativa

Gen  30: Max Fitness =  68.33 | Avg =  63.53 | Std =  5.04
         ‚Üí Cerca del √≥ptimo, poblaci√≥n homog√©nea

Gen  40: Max Fitness =  68.33 | Avg =  64.39 | Std =  6.31
         ‚Üí Sin mejora por 15 generaciones ‚Üí EARLY STOP
```

#### An√°lisis del Mejor Individuo:

```python
best_individual = [0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,...]
                                    ‚Üë              ‚Üë              ‚Üë
                                Feature 8      Feature 16     Feature 24
                                (Cz, Nodo 0)   (C4, Nodo 0)   (P3, Nodo 0)
```

**Interpretaci√≥n**: El AG seleccion√≥ caracter√≠sticas de nodos de baja frecuencia (Nodo 0) en canales centrales y parietales.

---

### 4.4 FASE 4: Entrenamiento del Clasificador

#### C√≥digo:
```python
X_cal_opt = X_cal_twp[:, selected_indices]  # (60, 4) si 4 features

clf = LinearDiscriminantAnalysis(shrinkage='auto', solver='eigen')
clf_final, metrics_cal = train_clf_and_get_metrics(
    X_cal_opt, 
    y_calibration, 
    clf
)

print(f"Calibraci√≥n - Acc: {metrics_cal.acc}% | TPR: {metrics_cal.tpr}%")
```

#### Proceso Interno:

1. **Reducci√≥n de Dimensionalidad**:
   - Entrada: `(60, 40)` ‚Üí Salida: `(60, 4)`
   - Se usan solo las columnas seleccionadas por el AG

2. **Entrenamiento LDA**:
   ```python
   clf.fit(X_cal_opt, y_calibration)
   ```
   
   Internamente:
   - Calcula medias por clase: `Œº_MI`, `Œº_Rest`
   - Calcula matrices de covarianza: `Œ£_MI`, `Œ£_Rest`
   - Aplica shrinkage: `Œ£_shrunk = (1-Œ±)Œ£ + Œ±¬∑I`
   - Resuelve problema de autovalores para encontrar `w`

3. **M√©tricas en Calibraci√≥n**:
   ```python
   y_pred = clf.predict(X_cal_opt)
   
   accuracy = np.sum(y_pred == y_true) / len(y_true) √ó 100
   
   tpr = np.sum(y_pred[y_true == 1] == 1) / np.sum(y_true == 1) √ó 100
   ```

---

### 4.5 FASE 5: Evaluaci√≥n en Terapia

#### C√≥digo:
```python
# 1. Extraer caracter√≠sticas de datos de terapia
X_ter_twp = get_twp_feature_vectors(
    data_ter, 
    level=3, 
    wavelet='coif3'
)  # (60, 40)

# 2. Aplicar LA MISMA selecci√≥n de caracter√≠sticas
X_ter_opt = X_ter_twp[:, selected_indices]  # (60, 4)

# 3. Clasificar con el modelo entrenado
y_pred = clf_final.predict(X_ter_opt)

# 4. Evaluar contra targets reales
metrics_ter = evaluate_clf_and_get_metrics(
    X_ter_opt, 
    clf_final, 
    targets_ter  # Targets reales de terapia
)

print(f"Terapia - Acc: {metrics_ter.acc}% | TPR: {metrics_ter.tpr}%")
```

#### ‚ö†Ô∏è IMPORTANTE: Mismo Pipeline de Preprocesamiento

Es **cr√≠tico** que los datos de terapia pasen por el **mismo pipeline**:

1. ‚úÖ Mismo filtro (8-30 Hz, Butterworth orden 4)
2. ‚úÖ Misma wavelet (coif3 por ejemplo)
3. ‚úÖ Mismo level (3)
4. ‚úÖ **Mismas caracter√≠sticas** (√≠ndices del AG)

Si cambias cualquiera de estos pasos, los resultados ser√°n inv√°lidos.

---

## 5. Configuraci√≥n y Par√°metros

### 5.1 Par√°metros del Pipeline

```python
# ============== TRANSFORMADA WAVELET PACKET ==============
TWP_LEVEL = 3              # Nivel de descomposici√≥n
                           # 3 ‚Üí 40 features | 4 ‚Üí 80 features

TWP_WAVELET = 'coif3'      # Familia de wavelet
                           # Opciones: 'db4', 'coif3', 'sym4', 'bior3.3'

# ============== ALGORITMO GEN√âTICO ==============
POP_SIZE = 100             # Tama√±o de la poblaci√≥n
                           # Rango recomendado: 50-200

N_GEN = 100                # N√∫mero m√°ximo de generaciones
                           # T√≠picamente converge en 30-60

LAMBDA_PENALTY = 1.0       # Peso de penalizaci√≥n por sparsity
                           # ‚Üë Mayor ‚Üí Menos caracter√≠sticas
                           # ‚Üì Menor ‚Üí M√°s caracter√≠sticas
                           # Rango: 0.3 - 2.0

EARLY_STOPPING = 15        # Paciencia para early stopping
                           # Detiene si no mejora en N generaciones

K_FOLDS = 5                # Folds para validaci√≥n cruzada
                           # Usado en el fitness del AG

# ============== CLASIFICADOR LDA ==============
SHRINKAGE = 'auto'         # Regularizaci√≥n autom√°tica
SOLVER = 'eigen'           # Solver m√°s estable

# ============== PREPROCESAMIENTO ==============
FILTER_LOW = 8             # Frecuencia baja del filtro (Hz)
FILTER_HIGH = 30           # Frecuencia alta del filtro (Hz)
FILTER_ORDER = 4           # Orden del filtro Butterworth
SAMPLING_RATE = 250        # Frecuencia de muestreo (Hz)
```

### 5.2 Gu√≠a de Ajuste de Par√°metros

#### Lambda (LAMBDA_PENALTY)

| Valor | Efecto | Cu√°ndo Usar |
|-------|--------|-------------|
| 0.1-0.3 | Selecci√≥n **permisiva** (10-15 features) | Dataset peque√±o, se√±ales ruidosas |
| 0.5-1.0 | Selecci√≥n **balanceada** (5-8 features) | **Recomendado** para inicio |
| 1.5-2.5 | Selecci√≥n **agresiva** (2-4 features) | Muchas caracter√≠sticas redundantes |
| >3.0 | **Demasiado restrictivo** (1-2 features) | ‚ö†Ô∏è No recomendado (underfitting) |

**Regla pr√°ctica:**
```
Si (Acc_Calibraci√≥n - Acc_Terapia) > 20%:
    ‚Üí Reducir lambda (menos overfitting)

Si n_caracter√≠sticas_promedio < 3:
    ‚Üí Reducir lambda (m√°s caracter√≠sticas)

Si TPR_Terapia < 50%:
    ‚Üí Reducir lambda o aumentar level
```

#### Level de TWP

| Level | N¬∞ Features | Resoluci√≥n Frecuencia | Uso Recomendado |
|-------|-------------|----------------------|-----------------|
| 2 | 20 (5√ó4) | Baja | Pruebas r√°pidas |
| **3** | **40 (5√ó8)** | **Media** | **Est√°ndar** |
| **4** | **80 (5√ó16)** | **Alta** | Mayor precisi√≥n |
| 5 | 160 (5√ó32) | Muy alta | ‚ö†Ô∏è Riesgo overfitting |

---

## 6. Resultados Esperados

### 6.1 M√©tricas T√≠picas

#### Calibraci√≥n (Training)
- **Accuracy**: 70-85%
- **TPR**: 75-90%
- **Gap calibraci√≥n-terapia**: 10-20%

#### Terapia (Test/Validaci√≥n)
- **Accuracy**: 55-70%
- **TPR**: 55-75%

#### Selecci√≥n de Caracter√≠sticas
- **Promedio seleccionadas**: 3-8 de 40 (7.5-20%)
- **Reducci√≥n**: 80-92.5%

### 6.2 Ejemplo de Resultados (Tu Ejecuci√≥n)

```
RESUMEN ESTAD√çSTICO:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
M√©trica                        Media        Std
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Caracter√≠sticas seleccionadas  2.88         1.36
Porcentaje selecci√≥n (%)       7.19         3.39
Fitness AG                     68.23        6.38
Acc Calibraci√≥n (%)            74.17        8.20
TPR Calibraci√≥n (%)            79.58        8.66
Acc Terapia (%)                56.88        6.38
TPR Terapia (%)                60.25        29.37
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
```

### 6.3 Comparaci√≥n con Baseline (Sin AG)

| M√©trica | Baseline (40 features) | Con AG (2.9 features) | Mejora |
|---------|------------------------|----------------------|--------|
| Acc Calibraci√≥n | 82.29% | 74.17% | -8.12% |
| TPR Calibraci√≥n | 85.42% | 79.58% | -5.84% |
| **Acc Terapia** | **55.83%** | **56.88%** | **+1.05%** ‚úÖ |
| **TPR Terapia** | **57.71%** | **60.25%** | **+2.54%** ‚úÖ |
| **Gap Calib-Terap** | **26.46%** | **17.29%** | **-9.17%** ‚úÖ |

**Conclusi√≥n:** El AG reduce el overfitting significativamente (-9.17% en gap) y mejora ligeramente la generalizaci√≥n en terapia (+1-2%).

---

## 7. Interpretaci√≥n de Resultados

### 7.1 An√°lisis de Canales Seleccionados

#### Distribuci√≥n Observada (Promedio 8 sujetos)

| Canal | Veces Seleccionado | % | Interpretaci√≥n Neurol√≥gica |
|-------|-------------------|---|---------------------------|
| **Cz** | 7/8 | 30.4% | **Corteza motora central** - Activaci√≥n motora general |
| **Pz** | 6/8 | 26.1% | **Corteza parietal** - Integraci√≥n sensoriomotora |
| **C4** | 5/8 | 21.7% | **Motor derecho** - Movimiento mano izquierda |
| **P3** | 5/8 | 21.7% | **Parietal izquierdo** - Procesamiento espacial |
| **C3** | 1/8 | 4.3% | Motor izquierdo - Menos relevante |

#### Montaje de Electrodos

```
        Cz (Centro motor)
         ‚îÇ
    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    ‚îÇ    ‚îÇ    ‚îÇ
   C3   Cz   C4  ‚Üê Fila motora
    ‚îÇ    ‚îÇ    ‚îÇ
   P3   Pz   P4  ‚Üê Fila parietal
```

**Interpretaci√≥n:**
- ‚úÖ **Cz y Pz dominan**: Esto es esperado, ya que son las √°reas centrales de activaci√≥n motora
- ‚ö†Ô∏è **C3 poco usado**: Puede indicar que el protocolo enfatiza movimiento de mano derecha o que C3 tiene m√°s ruido
- ‚úÖ **C4 importante**: Sugiere lateralizaci√≥n correcta (hemisferio derecho controla mano izquierda)

### 7.2 An√°lisis de Nodos Wavelet (Bandas de Frecuencia)

#### Distribuci√≥n por Nodo (Level 3 = 8 nodos)

```
Nodo  Frecuencia Aproximada    Veces Seleccionado
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  0   8-11 Hz (Mu bajo)        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (9)
  1   11-14 Hz (Mu alto/Beta)  ‚ñà‚ñà‚ñà‚ñà (4)
  2   14-17 Hz (Beta bajo)     ‚ñà‚ñà (2)
  3   17-20 Hz (Beta medio)    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (6)
  4   20-23 Hz (Beta alto)     ‚ñà‚ñà‚ñà‚ñà (4)
  5   23-26 Hz (Beta muy alto) ‚ñà‚ñà (2)
  6   26-29 Hz (Beta extremo)  ‚ñà‚ñà (2)
  7   29-30 Hz (l√≠mite Beta)   ‚ñà (1)
```

**Conclusi√≥n:**
- ‚úÖ **Nodo 0 (8-11 Hz)**: Ritmo mu dominante ‚Üí **Correcto**
- ‚úÖ **Nodo 3 (17-20 Hz)**: Beta medio tambi√©n importante
- ‚ö†Ô∏è **Nodos 5-7 poco usados**: Frecuencias altas menos informativas (m√°s ruido)

#### Relaci√≥n con ERD/ERS

Durante imager√≠a motora:
- **ERD (8-12 Hz)** ‚Üí Nodos 0-1 ‚Üí ‚úÖ Altamente seleccionados
- **ERD (18-26 Hz)** ‚Üí Nodos 3-5 ‚Üí ‚úÖ Moderadamente seleccionados
- **Ruido (>26 Hz)** ‚Üí Nodos 6-7 ‚Üí ‚úÖ Correctamente ignorados

### 7.3 An√°lisis de Sujetos Individuales

#### Sujetos con Buen Desempe√±o (TPR Terapia >75%)

**Sujeto 2:**
```
Caracter√≠sticas: 2/40 (5%)
- C4, Nodo 0 (8-11 Hz)
- Pz, Nodo 5 (23-26 Hz)

Calibraci√≥n: Acc=81.67%, TPR=93.33%
Terapia:     Acc=63.33%, TPR=91.67%

‚úÖ An√°lisis: Excelente TPR en ambas fases. La combinaci√≥n C4+Pz
            captura bien la lateralizaci√≥n motora.
```

**Sujeto 6:**
```
Caracter√≠sticas: 5/40 (12.5%)
- Cz, Nodo 0, 1
- C4, Nodo 0
- Pz, Nodo 2, 6

Calibraci√≥n: Acc=90.00%, TPR=86.67%
Terapia:     Acc=58.33%, TPR=90.62%

‚úÖ An√°lisis: Mayor n√∫mero de caracter√≠sticas permite capturar m√°s
            patrones. TPR excepcional (>90%) indica baja tasa de
            falsos negativos.
```

#### Sujetos Problem√°ticos (TPR Terapia <20%)

**Sujeto 3:**
```
Caracter√≠sticas: 2/40 (5%)
- P3, Nodo 0
- Pz, Nodo 3

Calibraci√≥n: Acc=68.33%, TPR=83.33%
Terapia:     Acc=50.00%, TPR=14.29%

‚ùå An√°lisis: Colapso del TPR en terapia (83% ‚Üí 14%). Posibles causas:
  1. Overfitting severo a datos de calibraci√≥n
  2. Distribuci√≥n diferente en terapia
  3. Solo 2 caracter√≠sticas insuficientes
  4. Caracter√≠sticas seleccionadas no generalizan

üí° Soluci√≥n: Reducir lambda (0.3-0.5) para seleccionar m√°s caracter√≠sticas
```

**Sujeto 4:**
```
Caracter√≠sticas: 2/40 (5%)
- Cz, Nodo 4
- P3, Nodo 6

Calibraci√≥n: Acc=70.00%, TPR=66.67%
Terapia:     Acc=55.00%, TPR=0.00%

‚ùå An√°lisis: TPR = 0% significa que el modelo clasifica TODOS los
            trials como clase Rest (nunca predice MI).

üí° Posibles causas:
  1. Umbral del clasificador muy alto
  2. Caracter√≠sticas no discriminativas
  3. Datos de terapia muy diferentes de calibraci√≥n
  4. Bias del modelo hacia clase mayoritaria

üí° Soluci√≥n: 
  - Usar m√°s caracter√≠sticas (lambda = 0.3)
  - Verificar balance de clases en targets_ter
  - Probar con level=4 (m√°s resoluci√≥n)
```

**Sujeto 8:**
```
Caracter√≠sticas: 1/40 (2.5%) ‚ö†Ô∏è
- C3, Nodo 0

Calibraci√≥n: Acc=61.67%, TPR=80.00%
Terapia:     Acc=46.67%, TPR=72.73%

‚ùå An√°lisis: Solo 1 caracter√≠stica es insuficiente. Aunque el TPR
            no colapsa, el accuracy es apenas mejor que azar (50%).

üí° Soluci√≥n: Forzar m√≠nimo de 3 caracter√≠sticas o reducir lambda a 0.3
```

### 7.4 M√©tricas de Calidad del Modelo

#### Gap de Generalizaci√≥n

```
Gap = Acc_Calibraci√≥n - Acc_Terapia

Gap Ideal:     5-10%  ‚Üí Excelente generalizaci√≥n
Gap Aceptable: 10-20% ‚Üí Generalizaci√≥n moderada
Gap Alto:      >20%   ‚Üí Overfitting severo
```

**Tu resultado:** 17.29% ‚Üí **Aceptable**, pero mejorable

#### Relaci√≥n Caracter√≠sticas vs. Performance

```python
# An√°lisis de correlaci√≥n (datos de tu ejecuci√≥n)

N_Features: [4, 2, 2, 2, 4, 5, 3, 1]
Acc_Terapia: [66.67, 63.33, 50.00, 55.00, 55.00, 58.33, 60.00, 46.67]

Correlaci√≥n: r = 0.61 (positiva moderada)
```

**Conclusi√≥n:** M√°s caracter√≠sticas ‚Üí Mejor accuracy (hasta cierto punto)

---

## 8. Gu√≠a de Troubleshooting

### 8.1 Problemas Comunes y Soluciones

#### Problema 1: TPR muy bajo en terapia (<40%)

**S√≠ntomas:**
```
Terapia: Acc=50-60%, TPR=0-30%
```

**Diagn√≥stico:**
- El modelo est√° sesgado hacia clase negativa (Rest)
- Caracter√≠sticas no capturan patrones de MI

**Soluciones:**
```python
# Soluci√≥n 1: Reducir lambda
LAMBDA_PENALTY = 0.3  # Antes: 1.0

# Soluci√≥n 2: Forzar m√≠nimo de caracter√≠sticas
MIN_FEATURES = 5

# Soluci√≥n 3: Aumentar resoluci√≥n
TWP_LEVEL = 4  # De 3 a 4 ‚Üí 40 a 80 features

# Soluci√≥n 4: Probar otra wavelet
TWP_WAVELET = 'db4'  # Daubechies en lugar de coif3

# Soluci√≥n 5: Verificar balance de clases
print(np.bincount(targets_ter))  # Debe ser [30, 30] aprox
```

---

#### Problema 2: Gap muy alto (>25%)

**S√≠ntomas:**
```
Calibraci√≥n: Acc=85%, TPR=90%
Terapia:     Acc=55%, TPR=60%
Gap = 30% ‚ùå
```

**Diagn√≥stico:** Overfitting severo

**Soluciones:**
```python
# Soluci√≥n 1: Aumentar penalizaci√≥n
LAMBDA_PENALTY = 1.5  # Forzar menos caracter√≠sticas

# Soluci√≥n 2: Usar validaci√≥n estratificada
# En genetic_algorithm.py:
from sklearn.model_selection import StratifiedKFold
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Soluci√≥n 3: Regularizaci√≥n adicional en LDA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
clf = LinearDiscriminantAnalysis(shrinkage=0.8)  # Forzar shrinkage

# Soluci√≥n 4: Data augmentation (avanzado)
# Aplicar jitter temporal o rotaci√≥n de canales
```

---

#### Problema 3: AG no converge (Fitness oscila)

**S√≠ntomas:**
```
Gen   0: Max=35.83
Gen  20: Max=45.12
Gen  40: Max=42.88  ‚Üê Retroceso
Gen  60: Max=48.00
Gen  80: Max=44.50  ‚Üê Oscilaci√≥n
```

**Diagn√≥stico:** 
- Poblaci√≥n demasiado peque√±a
- Tasa de mutaci√≥n muy alta
- Lambda mal ajustado

**Soluciones:**
```python
# Soluci√≥n 1: Aumentar poblaci√≥n
POP_SIZE = 200  # De 100 a 200

# Soluci√≥n 2: Reducir mutaci√≥n
# En genetic_algorithm.py:
toolbox.register("mutate", tools.mutFlipBit, indpb=0.03)  # De 0.05 a 0.03

# Soluci√≥n 3: Ajustar operadores gen√©ticos
# Aumentar cruce, reducir mutaci√≥n
CXPB = 0.8  # Probabilidad de cruce
MUTPB = 0.2  # Probabilidad de mutaci√≥n
```

---

#### Problema 4: Solo 1-2 caracter√≠sticas seleccionadas

**S√≠ntomas:**
```
Sujeto 8: 1/40 caracter√≠sticas (2.5%)
Fitness = 57.50
Acc_Terapia = 46.67% (apenas mejor que azar)
```

**Diagn√≥stico:** Lambda demasiado alta

**Soluciones:**
```python
# Soluci√≥n 1: Reducir lambda dr√°sticamente
LAMBDA_PENALTY = 0.3  # De 1.0 a 0.3

# Soluci√≥n 2: Forzar m√≠nimo de caracter√≠sticas
# En genetic_algorithm.py, funci√≥n evaluate_features:

MIN_FEATURES = 3
if n_selected < MIN_FEATURES:
    # Penalizar fitness
    penalty_additional = 20 * (MIN_FEATURES - n_selected)
    fitness_score -= penalty_additional

# Soluci√≥n 3: Cambiar funci√≥n de penalizaci√≥n
# Penalizaci√≥n logar√≠tmica en lugar de lineal:
penalty = lambda_penalty * np.log1p(n_selected) * 10
```

---

#### Problema 5: Error "singular matrix" en LDA

**S√≠ntomas:**
```
LinAlgError: Singular matrix
```

**Diagn√≥stico:** 
- M√°s caracter√≠sticas que muestras
- Covarianza no invertible

**Soluciones:**
```python
# Soluci√≥n 1: Ya est√°s usando shrinkage='auto' ‚úÖ

# Soluci√≥n 2: Forzar shrinkage m√°s alto
clf = LinearDiscriminantAnalysis(shrinkage=0.9, solver='eigen')

# Soluci√≥n 3: Reducir caracter√≠sticas m√°ximas
# En genetic_algorithm.py:
MAX_FEATURES = 15
if n_selected > MAX_FEATURES:
    return (0.0,)  # Penalizar individuos con >15 features
```

---

### 8.2 Checklist de Validaci√≥n

Antes de considerar los resultados finales, verifica:

- [ ] **Datos cargados correctamente**
  ```python
  print(dict_cal['subject_1']['mi_rest'].shape)  # Debe ser (625, 5, 60)
  print(dict_ter['subject_1']['mi'].shape)       # Debe ser (625, 5, 60)
  ```

- [ ] **Targets balanceados**
  ```python
  print(np.bincount(y_calibration))  # Debe ser [30, 30]
  print(np.bincount(targets_ter))    # Debe ser ~[30, 30]
  ```

- [ ] **Mismas caracter√≠sticas en train/test**
  ```python
  assert X_cal_opt.shape[1] == X_ter_opt.shape[1]
  ```

- [ ] **AG converge**
  ```python
  # Debe haber early stopping o llegar a max generaciones
  # Fitness max debe ser > 50
  ```

- [ ] **Caracter√≠sticas seleccionadas razonables**
  ```python
  # Entre 3 y 15 caracter√≠sticas
  assert 3 <= len(selected_indices) <= 15
  ```

- [ ] **M√©tricas dentro de rangos esperados**
  ```python
  assert 50 <= metrics_cal.acc <= 95  # Calibraci√≥n
  assert 45 <= metrics_ter.acc <= 75  # Terapia
  ```

---

## 9. Experimentos Avanzados

### 9.1 Grid Search de Hiperpar√°metros

```python
# Experimentar con diferentes configuraciones

param_grid = {
    'lambda': [0.3, 0.5, 1.0, 1.5],
    'level': [3, 4],
    'wavelet': ['db4', 'coif3', 'sym4'],
    'pop_size': [50, 100, 200]
}

best_config = None
best_acc = 0

for lam in param_grid['lambda']:
    for level in param_grid['level']:
        for wavelet in param_grid['wavelet']:
            # Entrenar con esta configuraci√≥n
            # ... (c√≥digo de pipeline)
            
            if acc_terapia > best_acc:
                best_acc = acc_terapia
                best_config = {
                    'lambda': lam,
                    'level': level,
                    'wavelet': wavelet
                }

print(f"Mejor configuraci√≥n: {best_config}")
print(f"Accuracy terapia: {best_acc:.2f}%")
```

### 9.2 Validaci√≥n Cruzada Sujeto-Independiente

```python
# Leave-One-Subject-Out Cross-Validation (LOSO-CV)

resultados_loso = []

for test_subject in range(1, 9):
    # Entrenar con 7 sujetos
    train_subjects = [s for s in range(1, 9) if s != test_subject]
    
    X_train = []
    y_train = []
    for s in train_subjects:
        X_s = get_twp_feature_vectors(dict_cal[f'subject_{s}']['mi_rest'])
        X_train.append(X_s)
        y_train.extend(y_calibration)
    
    X_train = np.vstack(X_train)
    
    # Aplicar AG
    best_ind, _ = run_genetic_algorithm(X_train, y_train, ...)
    selected = [i for i, b in enumerate(best_ind) if b == 1]
    
    # Entrenar LDA
    clf = LinearDiscriminantAnalysis(shrinkage='auto', solver='eigen')
    clf.fit(X_train[:, selected], y_train)
    
    # Evaluar en sujeto de test
    X_test = get_twp_feature_vectors(dict_cal[f'subject_{test_subject}']['mi_rest'])
    y_pred = clf.predict(X_test[:, selected])
    acc = accuracy(y_pred, y_calibration)
    
    resultados_loso.append(acc)
    print(f"Sujeto {test_subject} (test): {acc:.2f}%")

print(f"\nPromedio LOSO-CV: {np.mean(resultados_loso):.2f}%")
```

### 9.3 An√°lisis de Estabilidad de Caracter√≠sticas

```python
# ¬øQu√© caracter√≠sticas se seleccionan consistentemente?

feature_counts = np.zeros(40)

for sujeto in range(1, 9):
    data_cal = dict_cal[f'subject_{sujeto}']['mi_rest']
    X_cal = get_twp_feature_vectors(data_cal, level=3, wavelet='coif3')
    
    best_ind, _ = run_genetic_algorithm(X_cal, y_calibration, ...)
    
    for idx, bit in enumerate(best_ind):
        if bit == 1:
            feature_counts[idx] += 1

# Caracter√≠sticas m√°s estables (seleccionadas en ‚â•5 sujetos)
stable_features = np.where(feature_counts >= 5)[0]

print("Caracter√≠sticas estables (seleccionadas en ‚â•5 sujetos):")
for feat in stable_features:
    canal = feat // 8
    nodo = feat % 8
    print(f"  Feature {feat}: Canal {channel_names[canal]}, Nodo {nodo}")
    print(f"    Seleccionada en {feature_counts[feat]:.0f}/8 sujetos")
```

### 9.4 Comparaci√≥n con Otros M√©todos de Selecci√≥n

```python
from sklearn.feature_selection import SelectKBest, f_classif, RFE

# M√©todo 1: ANOVA F-test
selector_anova = SelectKBest(f_classif, k=5)
X_anova = selector_anova.fit_transform(X_cal_twp, y_calibration)

# M√©todo 2: Recursive Feature Elimination
clf_temp = LinearDiscriminantAnalysis(shrinkage='auto', solver='eigen')
selector_rfe = RFE(clf_temp, n_features_to_select=5)
X_rfe = selector_rfe.fit_transform(X_cal_twp, y_calibration)

# M√©todo 3: AG (tu m√©todo)
# ... (c√≥digo existente)

# Comparar en terapia
results_comparison = {
    'ANOVA': evaluate_clf(X_ter_anova, ...),
    'RFE': evaluate_clf(X_ter_rfe, ...),
    'AG': evaluate_clf(X_ter_ag, ...)
}

print("Comparaci√≥n de m√©todos:")
for method, acc in results_comparison.items():
    print(f"  {method}: {acc:.2f}%")
```

---

## 10. Extensiones Futuras

### 10.1 Mejoras del Pipeline

1. **Ensemble de Clasificadores**
   ```python
   from sklearn.ensemble import VotingClassifier
   
   clf1 = LinearDiscriminantAnalysis(shrinkage='auto')
   clf2 = SVC(kernel='rbf', probability=True)
   clf3 = RandomForestClassifier(n_estimators=100)
   
   ensemble = VotingClassifier(
       estimators=[('lda', clf1), ('svc', clf2), ('rf', clf3)],
       voting='soft'
   )
   ```

2. **Transfer Learning entre Sujetos**
   - Entrenar modelo base con todos los sujetos
   - Fine-tuning con datos del sujeto espec√≠fico

3. **Adaptaci√≥n Online**
   - Actualizar clasificador durante sesi√≥n de terapia
   - Usar feedback del usuario para corregir predicciones

### 10.2 Nuevas Caracter√≠sticas

1. **Common Spatial Patterns (CSP)**
   ```python
   from mne.decoding import CSP
   
   csp = CSP(n_components=4)
   X_csp = csp.fit_transform(data_cal, y_calibration)
   ```

2. **Conectividad Funcional**
   - Phase Locking Value (PLV)
   - Coherencia entre canales

3. **Features Temporales**
   - Hjorth Parameters (Activity, Mobility, Complexity)
   - Sample Entropy
   - Fractal Dimension

### 10.3 Clasificadores Alternativos

1. **Deep Learning**
   ```python
   # CNN para se√±ales EEG
   from tensorflow.keras import Sequential, layers
   
   model = Sequential([
       layers.Conv1D(32, kernel_size=5, activation='relu'),
       layers.MaxPooling1D(2),
       layers.Conv1D(64, kernel_size=3, activation='relu'),
       layers.GlobalAveragePooling1D(),
       layers.Dense(1, activation='sigmoid')
   ])
   ```

2. **Riemannian Geometry**
   ```python
   from pyriemann.classification import MDM
   
   # Minimum Distance to Mean en variedad de matrices de covarianza
   clf_riemann = MDM()
   ```

---

## 11. Conclusiones

### 11.1 Resumen del Pipeline

Este pipeline implementa un **enfoque h√≠brido estado del arte** para clasificaci√≥n de imager√≠a motora:

‚úÖ **Fortalezas:**
- Extracci√≥n robusta de caracter√≠sticas (TWP)
- Selecci√≥n inteligente guiada por evoluci√≥n (AG)
- Clasificador eficiente y estable (LDA)
- Reducci√≥n dr√°stica de dimensionalidad (92%)
- Prevenci√≥n de overfitting mediante sparsity

‚ö†Ô∏è **Limitaciones:**
- Requiere ajuste de hiperpar√°metros (lambda)
- Variabilidad inter-sujeto alta
- Computacionalmente costoso (AG tarda 5-10 min/sujeto)

### 11.2 Lecciones Aprendidas

1. **Lambda es cr√≠tico**: Controla el trade-off precisi√≥n vs. generalizaci√≥n
2. **Canales centrales (Cz, Pz) son los m√°s informativos**
3. **Ritmo mu (8-12 Hz) domina la discriminaci√≥n MI vs Rest**
4. **2-3 caracter√≠sticas pueden ser insuficientes** ‚Üí Aumentar m√≠nimo a 5
5. **Algunos sujetos requieren configuraciones espec√≠ficas**

### 11.3 Recomendaciones Finales

Para **uso en producci√≥n**:
```python
# Configuraci√≥n recomendada basada en an√°lisis
TWP_LEVEL = 4           # Mayor resoluci√≥n
TWP_WAVELET = 'coif3'   # Mantener
POP_SIZE = 100          # Suficiente
N_GEN = 100             # Con early stopping
LAMBDA_PENALTY = 0.5    # Balanceado
EARLY_STOPPING = 15     # Mantener
MIN_FEATURES = 5        # Nuevo: forzar m√≠nimo
```

Para **investigaci√≥n**:
- Experimentar con nivel 5 (160 features)
- Probar wavelets adaptativas
- Implementar multi-objetivo AG (precisi√≥n + robustez)
- Validaci√≥n LOSO-CV obligatoria

---

## 12. Referencias

### 12.1 Literatura Cient√≠fica

1. **Imager√≠a Motora y BCI:**
   - Pfurtscheller, G., & Neuper, C. (2001). *Motor imagery and direct brain-computer communication.* IEEE, 89(7), 1123-1134.
   - Blankertz, B., et al. (2008). *The BCI competition III: Validating alternative approaches to actual BCI problems.* IEEE Trans Neural Syst Rehabil Eng, 14(2), 153-159.

2. **Transformada Wavelet:**
   - Subasi, A. (2007). *EEG signal classification using wavelet feature extraction and a mixture of expert model.* Expert Systems with Applications, 32(4), 1084-1093.
   - Mallat, S. (1989). *A theory for multiresolution signal decomposition: the wavelet representation.* IEEE Trans Pattern Anal Mach Intell, 11(7), 674-693.

3. **Algoritmos Gen√©ticos:**
   - Goldberg, D. E. (1989). *Genetic Algorithms in Search, Optimization, and Machine Learning.* Addison-Wesley.
   - Fortin, F. A., et al. (2012). *DEAP: Evolutionary algorithms made easy.* Journal of Machine Learning Research, 13, 2171-2175.

4. **Selecci√≥n de Caracter√≠sticas en BCI:**
   - Lotte, F., et al. (2018). *A review of classification algorithms for EEG-based brain-computer interfaces: a 10 year update.* J Neural Eng, 15(3), 031005.
   - Baig, M. Z., et al. (2017). *Filtering techniques for channel selection in motor imagery EEG applications: a survey.* Artif Intell Rev, 53, 1207-1232.

### 12.2 Herramientas y Librer√≠as

- **PyWavelets:** https://pywavelets.readthedocs.io/
- **DEAP (Genetic Algorithms):** https://deap.readthedocs.io/
- **Scikit-learn (LDA):** https://scikit-learn.org/
- **MNE-Python (EEG):** https://mne.tools/

### 12.3 Datasets

- **IM-tention Dataset:** [Dataset de creaci√≥n propia por el CIRINS - FiUNER]
---

## Ap√©ndices

### Ap√©ndice A: Glosario de T√©rminos

| T√©rmino | Definici√≥n |
|---------|-----------|
| **BCI** | Brain-Computer Interface - Interfaz cerebro-computadora |
| **ERD** | Event-Related Desynchronization - Desincronizaci√≥n relacionada con evento |
| **ERS** | Event-Related Synchronization - Sincronizaci√≥n relacionada con evento |
| **MI** | Motor Imagery - Imager√≠a motora |
| **TWP** | Tree Wavelet Packet - Transformada Wavelet Packet |
| **AG** | Algoritmo Gen√©tico |
| **LDA** | Linear Discriminant Analysis - An√°lisis Discriminante Lineal |
| **TPR** | True Positive Rate - Tasa de verdaderos positivos (Sensibilidad) |
| **Sparsity** | Escasez - Proporci√≥n de caracter√≠sticas seleccionadas |
| **Early Stopping** | Detenci√≥n temprana del entrenamiento |

### Ap√©ndice B: C√≥digo Completo M√≠nimo

```python
# Pipeline m√≠nimo en ~30 l√≠neas

import numpy as np
from modules.preprocessing.file_creation import create_mat_files
from modules.feature_extraction.feature_extraction import get_twp_feature_vectors
from modules.genetic_algorithm.genetic_algorithm import run_genetic_algorithm
from modules.training.training import train_clf_and_get_metrics
from modules.evaluation.evaluation import evaluate_clf_and_get_metrics
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# 1. Cargar datos
dict_cal = create_mat_files('./data/im_tention_signals', file_type='calibration', filtfilt=True)
dict_ter = create_mat_files('./data/im_tention_signals', file_type='therapy', ther_number_of_trials=60, filtfilt=True)

# 2. Preparar targets
y_calibration = np.hstack((np.ones(30, dtype=np.int8), np.zeros(30, dtype=np.int8)))

# 3. Pipeline para un sujeto
data_cal = dict_cal['subject_1']['mi_rest']
data_ter = dict_ter['subject_1']['mi']
targets_ter = dict_ter['subject_1']['target']

# 4. Extracci√≥n TWP
X_cal = get_twp_feature_vectors(data_cal, level=3, wavelet='coif3')
X_ter = get_twp_feature_vectors(data_ter, level=3, wavelet='coif3')

# 5. Selecci√≥n AG
best_ind, _ = run_genetic_algorithm(X_cal, y_calibration, pop_size=100, num_generations=100, lambda_penalty=0.5)
selected = [i for i, b in enumerate(best_ind) if b == 1]

# 6. Entrenamiento
clf = LinearDiscriminantAnalysis(shrinkage='auto', solver='eigen')
clf, metrics_cal = train_clf_and_get_metrics(X_